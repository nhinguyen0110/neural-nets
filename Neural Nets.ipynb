{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3wea+XFfP3YapMZokCB/B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ggt4mdxEt_9O"},"outputs":[],"source":["#@title Import relevant modules\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","# The following lines adjust the granularity of reporting.\n","pd.options.display.max_rows = 10\n","pd.options.display.float_format = \"{:.1f}\".format\n","\n","print(\"Imported modules.\")"]},{"cell_type":"code","source":["train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n","train_df = train_df.reindex(np.random.permutation(train_df.index)) # shuffle the examples\n","test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"],"metadata":{"id":"V025aT1pvCyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Keras Input tensors of float values.\n","inputs = {\n","    'latitude':\n","        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n","                              name='latitude'),\n","    'longitude':\n","        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n","                              name='longitude'),\n","    'median_income':\n","        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n","                              name='median_income'),\n","    'population':\n","        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n","                              name='population')\n","}\n","\n","# Create a Normalization layer to normalize the median_income data.\n","median_income = tf.keras.layers.Normalization(\n","    name='normalization_median_income',\n","    axis=None)\n","median_income.adapt(train_df['median_income'])\n","median_income = median_income(inputs.get('median_income'))\n","\n","# Create a Normalization layer to normalize the population data.\n","population = tf.keras.layers.Normalization(\n","    name='normalization_population',\n","    axis=None)\n","population.adapt(train_df['population'])\n","population = population(inputs.get('population'))\n","\n","# Create a list of numbers representing the bucket boundaries for latitude.\n","# Because we're using a Normalization layer, values for latitude and longitude\n","# will be in the range of approximately -3 to 3 (representing the Z score).\n","# We'll create 20 buckets, which requires 21 bucket boundaries (hence, 20+1).\n","latitude_boundaries = np.linspace(-3, 3, 20+1)\n","\n","# Create a Normalization layer to normalize the latitude data.\n","latitude = tf.keras.layers.Normalization(\n","    name='normalization_latitude',\n","    axis=None)\n","latitude.adapt(train_df['latitude'])\n","latitude = latitude(inputs.get('latitude'))\n","\n","# Create a Discretization layer to separate the latitude data into buckets.\n","latitude = tf.keras.layers.Discretization(\n","    bin_boundaries=latitude_boundaries,\n","    name='discretization_latitude')(latitude)\n","\n","# Create a list of numbers representing the bucket boundaries for longitude.\n","longitude_boundaries = np.linspace(-3, 3, 20+1)\n","\n","# Create a Normalization layer to normalize the longitude data.\n","longitude = tf.keras.layers.Normalization(\n","    name='normalization_longitude',\n","    axis=None)\n","longitude.adapt(train_df['longitude'])\n","longitude = longitude(inputs.get('longitude'))\n","\n","# Create a Discretization layer to separate the longitude data into buckets.\n","longitude = tf.keras.layers.Discretization(\n","    bin_boundaries=longitude_boundaries,\n","    name='discretization_longitude')(longitude)\n","\n","# Cross the latitude and longitude features into a single one-hot vector.\n","feature_cross = tf.keras.layers.HashedCrossing(\n","    # num_bins can be adjusted: Higher values improve accuracy, lower values\n","    # improve performance.\n","    num_bins=len(latitude_boundaries) * len(longitude_boundaries),\n","    output_mode='one_hot',\n","    name='cross_latitude_longitude')([latitude, longitude])\n","\n","# Concatenate our inputs into a single tensor.\n","preprocessing_layers = tf.keras.layers.Concatenate()(\n","    [feature_cross, median_income, population])\n","\n","print(\"Preprocessing layers defined.\")"],"metadata":{"id":"rMZw7SOVvFPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define the plotting function.\n","\n","def plot_the_loss_curve(epochs, mse_training, mse_validation):\n","  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n","\n","  plt.figure()\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Mean Squared Error\")\n","\n","  plt.plot(epochs, mse_training, label=\"Training Loss\")\n","  plt.plot(epochs, mse_validation, label=\"Validation Loss\")\n","\n","  # mse_training is a pandas Series, so convert it to a list first.\n","  merged_mse_lists = mse_training.tolist() + mse_validation\n","  highest_loss = max(merged_mse_lists)\n","  lowest_loss = min(merged_mse_lists)\n","  top_of_y_axis = highest_loss * 1.03\n","  bottom_of_y_axis = lowest_loss * 0.97\n","\n","  plt.ylim([bottom_of_y_axis, top_of_y_axis])\n","  plt.legend()\n","  plt.show()\n","\n","print(\"Defined the plot_the_loss_curve function.\")"],"metadata":{"id":"AhNGijiWvIMd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define functions to create and train a linear regression model\n","def create_model(my_inputs, my_outputs, my_learning_rate):\n","  \"\"\"Create and compile a simple linear regression model.\"\"\"\n","  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\n","\n","  # Construct the layers into a model that TensorFlow can execute.\n","  model.compile(optimizer=tf.keras.optimizers.Adam(\n","      learning_rate=my_learning_rate),\n","      loss=\"mean_squared_error\",\n","      metrics=[tf.keras.metrics.MeanSquaredError()])\n","\n","  return model\n","\n","# Create Normalization layers to normalize the median_house_value data.\n","# Because median_house_value is our label (i.e., the target value we're\n","# predicting), these layers won't be added to our model.\n","train_median_house_value_normalized = tf.keras.layers.Normalization(axis=None)\n","train_median_house_value_normalized.adapt(\n","    np.array(train_df['median_house_value']))\n","\n","test_median_house_value_normalized = tf.keras.layers.Normalization(axis=None)\n","test_median_house_value_normalized.adapt(\n","    np.array(test_df['median_house_value']))\n","\n","def train_model(model, dataset, epochs, batch_size, label_name, validation_split=0.1):\n","  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n","\n","  # Split the dataset into features and label.\n","  features = {name:np.array(value) for name, value in dataset.items()}\n","  label = train_median_house_value_normalized(\n","      np.array(features.pop(label_name)))\n","  history = model.fit(x=features, y=label, batch_size=batch_size,\n","                      epochs=epochs, shuffle=True, validation_split=validation_split)\n","\n","  # Get details that will be useful for plotting the loss curve.\n","  epochs = history.epoch\n","  hist = pd.DataFrame(history.history)\n","  mse = hist[\"mean_squared_error\"]\n","\n","  return epochs, mse, history.history\n","\n","print(\"Defined the create_model and train_model functions.\")"],"metadata":{"id":"TyYW7WEzvKlD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define linear regression model outputs\n","def get_outputs_linear_regression():\n","  # Create the Dense output layer.\n","  dense_output = tf.keras.layers.Dense(units=1, input_shape=(1,),\n","                              name='dense_output')(preprocessing_layers)\n","\n","  # Define an output dictionary we'll send to the model constructor.\n","  outputs = {\n","    'dense_output': dense_output\n","  }\n","  return outputs"],"metadata":{"id":"e7HyvwigvNwL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The following variables are the hyperparameters.\n","learning_rate = 0.01\n","epochs = 15\n","batch_size = 1000\n","label_name = \"median_house_value\"\n","\n","# Split the original training set into a reduced training set and a\n","# validation set.\n","validation_split = 0.2\n","\n","outputs = get_outputs_linear_regression()\n","\n","# Establish the model's topography.\n","my_model = create_model(inputs, outputs, learning_rate)\n","\n","# Train the model on the normalized training set.\n","epochs, mse, history = train_model(my_model, train_df, epochs, batch_size,\n","                          label_name, validation_split)\n","plot_the_loss_curve(epochs, mse, history[\"val_mean_squared_error\"])\n","\n","test_features = {name:np.array(value) for name, value in test_df.items()}\n","test_label = test_median_house_value_normalized(test_features.pop(label_name)) # isolate the label\n","print(\"\\n Evaluate the linear regression model against the test set:\")\n","my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size, return_dict=True)"],"metadata":{"id":"45VaDCd5vPue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_outputs_dnn():\n","  # Create a Dense layer with 20 nodes.\n","  dense_output = tf.keras.layers.Dense(units=20, input_shape=(1,),\n","                              activation='relu',\n","                              name='hidden_dense_layer_1')(preprocessing_layers)\n","  # Create a Dense layer with 12 nodes.\n","  dense_output = tf.keras.layers.Dense(units=12, input_shape=(1,),\n","                              activation='relu',\n","                              name='hidden_dense_layer_2')(dense_output)\n","  # Create the Dense output layer.\n","  dense_output = tf.keras.layers.Dense(units=1, input_shape=(1,),\n","                              name='dense_output')(dense_output)\n","\n","  # Define an output dictionary we'll send to the model constructor.\n","  outputs = {\n","    'dense_output': dense_output\n","  }\n","\n","  return outputs"],"metadata":{"id":"a27Ehd0MvSBm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The following variables are the hyperparameters.\n","learning_rate = 0.01\n","epochs = 20\n","batch_size = 1000\n","\n","# Specify the label\n","label_name = \"median_house_value\"\n","\n","# Split the original training set into a reduced training set and a\n","# validation set.\n","validation_split = 0.2\n","\n","dnn_outputs = get_outputs_dnn()\n","\n","# Establish the model's topography.\n","my_model = create_model(\n","    inputs,\n","    dnn_outputs,\n","    learning_rate)\n","\n","# Train the model on the normalized training set. We're passing the entire\n","# normalized training set, but the model will only use the features\n","# defined in our inputs.\n","epochs, mse, history = train_model(my_model, train_df, epochs,\n","                                   batch_size, label_name, validation_split)\n","plot_the_loss_curve(epochs, mse, history[\"val_mean_squared_error\"])\n","\n","# After building a model against the training set, test that model\n","# against the test set.\n","test_features = {name:np.array(value) for name, value in test_df.items()}\n","test_label = test_median_house_value_normalized(np.array(test_features.pop(label_name))) # isolate the label\n","print(\"\\n Evaluate the new model against the test set:\")\n","my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size, return_dict=True)"],"metadata":{"id":"kIVTZh9uvVaA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1zHz0B_DvZtc"},"execution_count":null,"outputs":[]}]}